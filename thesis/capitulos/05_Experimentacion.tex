\chapter{Experimentación}\label{cap:experimentacion}

\section{Experimentos preliminares}

En esta primera fase de experimentacion se realizarán pruebas para determinar los parámetros óptimos de ejecución y estudiar el rendimiento de la aplicación en distintos entornos y plataformas.

\subsection{Determinación del número óptimo de subpoblaciones y hebras}

Como se ha comentado en la sección \ref{subsubsec:determinacion_subpoblaciones}, se realizarán ejecuciones exploratorias con distintas configuraciones de subpoblaciones y hebras, para definir el parámetro de \texttt{NSubpopulations} a usar en los tests posteriores.

En la figura \ref{fig:exploratory_subpopulations} se muestran las gráficas de estas ejecuciones exploratorias. Se observa que para cualquier número de subpoblaciones, el comportamiento es similar. En la tabla \ref{tab:exploratory_subpopulations_times} se presentan los tiempos de ejecución en segundos para cada combinación de hebras y subpoblaciones, mostrando cómo el incremento en el número de subpoblaciones aumenta significativamente el tiempo de ejecución, especialmente cuando se utilizan pocas hebras.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/exploratory_subpopulations.png}
    \caption{Gráficas de ejecución de las pruebas variando el número de subpoblaciones y hebras.}
    \label{fig:exploratory_subpopulations}
\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|ccccc|}
        \hline
        \multirow{2}{*}{\textbf{Hebras}} & \multicolumn{5}{c|}{\textbf{Subpoblaciones}}                                                      \\ \cline{2-6}
                                         & \textbf{1}                                   & \textbf{2} & \textbf{4} & \textbf{8} & \textbf{16} \\ \hline
        1                                & 309.43                                       & 622.63     & 1239.73    & 2481.89    & 4960.00     \\ \hline
        2                                & 157.79                                       & 313.86     & 633.66     & 1252.12    & 2513.72     \\ \hline
        4                                & 82.73                                        & 169.76     & 336.25     & 680.46     & 1320.66     \\ \hline
        8                                & 50.58                                        & 99.37      & 200.01     & 398.66     & 790.04      \\ \hline
        16                               & 52.32                                        & 104.06     & 209.17     & 414.11     & 818.17      \\ \hline
    \end{tabular}
    \caption{Tiempos de ejecución en segundos de las pruebas variando el número de subpoblaciones y hebras.}
    \label{tab:exploratory_subpopulations_times}
\end{table}

En la tabla \ref{tab:exploratory_populations_delta} se presentan los porcentajes de reducción del tiempo de ejecución respecto a la configuración base. En cada columna, la ejecución con una sola hebra y una sola subpoblación se toma como referencia (0\% de reducción). Estos resultados permiten cuantificar de manera objetiva el beneficio relativo derivado del incremento del paralelismo y de la subdivisión de la población, proporcionando una base empírica para identificar configuraciones óptimas de ejecución.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|ccccc|}
        \hline
        \multirow{2}{*}{\textbf{Hebras}} & \multicolumn{5}{c|}{\textbf{Subpoblaciones}}                                                      \\ \cline{2-6}
                                         & \textbf{1}                                   & \textbf{2} & \textbf{4} & \textbf{8} & \textbf{16} \\ \hline
        1                                & 0.00                                         & 0.00       & 0.00       & 0.00       & 0.00        \\ \hline
        2                                & -49.01                                       & -49.59     & -48.89     & -49.55     & -49.32      \\ \hline
        4                                & -73.26                                       & -72.74     & -72.88     & -72.58     & -73.37      \\ \hline
        8                                & -83.65                                       & -84.04     & -83.87     & -83.94     & -84.07      \\ \hline
        16                               & -83.09                                       & -83.29     & -83.13     & -83.31     & -83.50      \\ \hline
    \end{tabular}
    \caption{Porcentaje de reducción del tiempo de ejecución respecto a la configuración base para distintas combinaciones de hebras y subpoblaciones}
    \label{tab:exploratory_populations_delta}
\end{table}

Del análisis de esta tabla pueden extraerse varias conclusiones de interés para la definición de los parámetros en los experimentos posteriores. En primer lugar, se observa que, para un número fijo de hebras, la variación en el porcentaje de reducción del tiempo es prácticamente inexistente al modificar el número de subpoblaciones, lo que indica que el comportamiento es proporcional con independencia de este parámetro. En segundo lugar, el mayor beneficio en términos de reducción del tiempo de ejecución se obtiene al incrementar el número de hebras de 1 a 8, alcanzando valores en torno al 83--84\%. Sin embargo, al pasar de 8 a 16 hebras, los tiempos de ejecución se incrementan en todos los casos, lo que revela que se ha sobrepasado el punto de paralelismo óptimo para la arquitectura hardware utilizada. Este resultado sugiere que, en el entorno experimental considerado, el uso de más de 8 hebras no proporciona mejoras de rendimiento adicionales e, incluso, puede resultar contraproducente debido a la sobrecarga y la contención de recursos.

No obstante, se ha decidido mantener configuraciones con 16 hebras en los experimentos posteriores con el fin de analizar el comportamiento bajo condiciones de sobrecarga, dado que el objetivo del estudio trasciende la mera optimización de un caso específico y busca evaluar la escalabilidad y el rendimiento en un rango amplio de escenarios. En cuanto al número de subpoblaciones, se constata que con 16 subpoblaciones los tiempos de ejecución pueden alcanzar hasta 1 hora y 20 minutos en configuraciones con una única hebra, lo que resulta excesivo para los objetivos de este trabajo. Por el contrario, con 8 subpoblaciones se obtienen tiempos de ejecución más razonables, se alcanza un equilibrio adecuado entre eficiencia y aprovechamiento de recursos, y se garantiza una buena escalabilidad.

En consecuencia, se justifica la decisión metodológica de fijar el número de subpoblaciones en 8 para los experimentos posteriores, al representar un compromiso óptimo entre eficiencia, utilización de recursos y escalabilidad en el entorno analizado.

\subsection{Estudio del rendimiento al requerir más hebras de las disponibles}

En esta sección se presentan los resultados de las ejecuciones exploratorias realizadas para analizar el rendimiento de la aplicación al variar el número de nodos y hebras, incluyendo configuraciones que superan el límite físico de hebras de la CPU (16 hebras). El objetivo es comprender cómo afecta esta variación al tiempo de ejecución y al uso efectivo de la CPU, proporcionando una base para la selección de parámetros en estudios posteriores.

En la figura \ref{fig:exploratory_threads_limit_time} se muestra la evolución del tiempo de ejecución en función del número de hebras asignadas por nodo, para configuraciones que van desde 1 hasta 16 nodos, pero limitando el número máximo de hebras a 16.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/exploratory_threads_limit_time.png}
    \caption{Gráfica de tiempo de ejecución en función del número de hebras por nodo, con límite de 16 hebras.}
    \label{fig:exploratory_threads_limit_time}
\end{figure}

La figura \ref{fig:exploratory_threads_limit_cpu} muestra el porcentaje de uso total de CPU (donde 100\% equivale al uso completo de una hebra) para las distintas configuraciones de hebras por nodo analizadas.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/exploratory_threads_limit_cpu.png}
    \caption{Gráfica de uso de CPU en función del número de hebras por nodo, con límite de 16 hebras.}
    \label{fig:exploratory_threads_limit_cpu}
\end{figure}

Por otro lado, en la figura \ref{fig:exploratory_threads_no-limit_time} se presentan los resultados de las ejecuciones exploratorias sin límite en el número de hebras, permitiendo así evaluar el comportamiento del sistema al requerir más hebras de las disponibles.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/exploratory_threads_no-limit_time.png}
    \caption{Gráfica de tiempo de ejecución en función del número de hebras por nodo, sin límite en el número de hebras.}
    \label{fig:exploratory_threads_no-limit_time}
\end{figure}

La figura \ref{fig:exploratory_threads_no-limit_cpu} muestra el porcentaje de uso total de CPU para las distintas configuraciones de hebras por nodo sin límite, permitiendo observar cómo varía el aprovechamiento de los recursos en función del número total de hebras asignadas.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/exploratory_threads_no-limit_cpu.png}
    \caption{Gráfica de uso de CPU en función del número de hebras por nodo, sin límite en el número de hebras.}
    \label{fig:exploratory_threads_no-limit_cpu}
\end{figure}

Las conclusiones que se pueden extraer de estas gráficas se pueden ver de manera más clara en la tabla \ref{tab:summary_nodes_threads_cpu}, donde se resumen los tiempos de ejecución y el uso de CPU para todas las combinaciones de nodos y hebras analizadas, tanto con límite como sin límite en el número de hebras.

\begin{table}[ht]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Hebras}   & \textbf{Hebras}  & \textbf{Tiempo} & \textbf{Uso de}   \\
                       & \textbf{por nodo} & \textbf{totales} & \textbf{(s)}    & \textbf{CPU (\%)} \\
        \hline
        1              & 8                 & 8                & 395.63          & 782               \\
        1              & 16                & 16               & 409.51          & 1561              \\
        4              & 16                & 64               & 431.30          & 1593              \\
        4              & 8                 & 32               & 443.23          & 1564              \\
        16             & 16                & 256              & 453.48          & 1586              \\
        8              & 16                & 128              & 463.45          & 1576              \\
        2              & 16                & 32               & 469.88          & 1460              \\
        8              & 8                 & 64               & 475.59          & 1570              \\
        4              & 4                 & 16               & 477.90          & 1063              \\
        16             & 8                 & 128              & 478.23          & 1597              \\
        16             & 4                 & 64               & 540.54          & 1588              \\
        2              & 8                 & 16               & 571.65          & 706               \\
        4              & 2                 & 8                & 573.94          & 659               \\
        8              & 4                 & 32               & 581.91          & 1412              \\
        16             & 2                 & 32               & 662.10          & 1598              \\
        1              & 4                 & 4                & 666.51          & 396               \\
        2              & 4                 & 8                & 719.43          & 494               \\
        8              & 2                 & 16               & 776.44          & 1151              \\
        8              & 1                 & 8                & 794.09          & 796               \\
        16             & 1                 & 16               & 868.28          & 1592              \\
        4              & 1                 & 4                & 956.38          & 399               \\
        2              & 2                 & 4                & 1263.74         & 299               \\
        1              & 2                 & 2                & 1264.94         & 199               \\
        1              & 1                 & 1                & 2467.76         & 99                \\
        2              & 1                 & 2                & 2497.06         & 199               \\
        \hline
    \end{tabular}
    \caption{Resumen de configuraciones de nodos, hebras y uso de CPU}
    \label{tab:summary_nodes_threads_cpu}
\end{table}

Los resultados muestran que el mejor rendimiento se alcanza empleando un menor número de nodos con un mayor número de hebras por nodo, siendo la configuración óptima la de un único nodo y ocho hebras. Aunque el límite físico de hebras de la CPU es de 16, se observa que, entre los diez mejores resultados, solo tres respetan este límite. En los demás casos, incrementar el número de hebras más allá de la capacidad física sigue proporcionando mejoras en el rendimiento, un comportamiento que, aunque inicialmente contraintuitivo, puede explicarse analizando el uso efectivo de la CPU.

El porcentaje de utilización de la CPU refleja el grado de aprovechamiento de las hebras disponibles: por ejemplo, con una hebra se alcanza un uso máximo de 100\%, con dos hebras el 200\%, y así sucesivamente hasta un máximo teórico de 1600\% (16 hebras $\times$ 100\%). En configuraciones de un único nodo, aumentar el número de hebras se traduce en un incremento proporcional del uso de CPU, lo que explica las mejoras de rendimiento observadas.

En contraste, cuando el número total de hebras se distribuye entre varios nodos, incluso si no se supera el límite físico de la CPU, el rendimiento no mejora de la misma manera. Esto se debe a la sobrecarga inherente a la gestión de múltiples nodos, que puede contrarrestar los beneficios de disponer de más hebras, haciendo que el uso efectivo de la CPU no alcance los valores esperados y resultando en un rendimiento inferior respecto a configuraciones mononodo equivalentes. Por tanto, para configuraciones multinodo es necesario incrementar el número de hebras más allá de la capacidad física de cada CPU para acercarse al uso máximo teórico, lo que explica por qué los mejores resultados se obtienen bajo estas condiciones.

\subsection{Estudio del rendimiento al utilizar la misma GPU en distintos nodos}

En esta sección se presentan los resultados de las ejecuciones exploratorias realizadas para analizar el rendimiento de la aplicación al variar el número de nodos y hebras, considerando dos enfoques distintos respecto al uso de la GPU: uno en el que se limita su uso a un único nodo y otro en el que se permite su uso en todos los nodos. El objetivo es comprender cómo afecta esta variación al tiempo de ejecución y al uso efectivo de la CPU, proporcionando una base para la selección de parámetros en estudios posteriores.

En la figura \ref{fig:exploratory_gpu_limit_time} se muestra la evolución del tiempo de ejecución en función del número de hebras asignadas por nodo, para configuraciones que van desde 1 hasta 16 nodos, limitando el uso de la GPU a un único nodo.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/exploratory_gpu_limit_time.png}
    \caption{Gráfica de tiempo de ejecución en función del número de hebras por nodo, con la GPU limitada a un único nodo.}
    \label{fig:exploratory_gpu_limit_time}
\end{figure}

La figura \ref{fig:exploratory_gpu_no-limit_time} presenta los resultados de las ejecuciones exploratorias permitiendo el uso de la GPU en todos los nodos, evaluando así el comportamiento del sistema bajo esta configuración.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/exploratory_gpu_no-limit_time.png}
    \caption{Gráfica de tiempo de ejecución en función del número de hebras por nodo, permitiendo el uso de la GPU en todos los nodos.}
    \label{fig:exploratory_gpu_no-limit_time}
\end{figure}

Las conclusiones que se pueden extraer de estas gráficas se pueden ver de manera más clara en la tabla \ref{tab:summary_nodes_threads_gpu}, donde se resumen los tiempos de ejecución para todas las combinaciones de nodos y hebras analizadas, tanto con la GPU limitada a un nodo como permitiendo su uso en todos los nodos.

\begin{table}[ht]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Hebras} & \textbf{GPU 1 nodo (s)} & \textbf{GPU todos (s)} & \textbf{Var. (\%)} \\
        \hline
        1              & 1               & 311.68                  & 319.01                 & 2.35               \\
        1              & 2               & 163.59                  & 158.94                 & -2.84              \\
        1              & 4               & 84.49                   & 88.52                  & 4.77               \\
        1              & 8               & 59.91                   & 59.41                  & -0.83              \\
        1              & 16              & 56.45                   & 56.91                  & 0.81               \\
        2              & 1               & 316.92                  & 317.35                 & 0.14               \\
        2              & 2               & 168.14                  & 170.99                 & 1.70               \\
        2              & 4               & 90.58                   & 91.78                  & 1.32               \\
        2              & 8               & 74.30                   & 75.53                  & 1.66               \\
        2              & 16              & 65.40                   & 64.51                  & -1.36              \\
        4              & 1               & 322.77                  & 341.95                 & 5.94               \\
        4              & 2               & 187.98                  & 199.04                 & 5.88               \\
        4              & 4               & 170.38                  & 174.74                 & 2.56               \\
        4              & 8               & 163.80                  & 167.31                 & 2.14               \\
        4              & 16              & 159.22                  & 164.07                 & 3.05               \\
        8              & 1               & 401.33                  & 410.54                 & 2.29               \\
        8              & 2               & 393.50                  & 385.47                 & -2.04              \\
        8              & 4               & 376.68                  & 271.10                 & -28.03             \\
        8              & 8               & 366.47                  & 244.26                 & -33.35             \\
        8              & 16              & 363.94                  & 228.34                 & -37.26             \\
        16             & 1               & 869.40                  & 867.50                 & -0.22              \\
        16             & 2               & 628.49                  & 563.51                 & -10.34             \\
        16             & 4               & 496.28                  & 404.98                 & -18.40             \\
        16             & 8               & 427.30                  & 312.94                 & -26.76             \\
        16             & 16              & 400.48                  & 264.49                 & -33.96             \\
        \hline
    \end{tabular}
    \caption{Resumen de tiempos de ejecución para distintas combinaciones de nodos y hebras, comparando el uso de la GPU limitada a un nodo frente a su uso en todos los nodos.}
    \label{tab:summary_nodes_threads_gpu}
\end{table}

Para configuraciones con pocos nodos (1, 2 o 4), la diferencia entre utilizar la GPU en un único nodo o en todos los nodos resulta pequeña y variable. Las variaciones porcentuales oscilan entre valores positivos y negativos, pero en general se mantienen por debajo del 6\%. Esto indica que, en estos escenarios, no existe una ventaja clara ni consistente de emplear la GPU de forma compartida en todos los nodos.

A partir de 8 nodos, la ejecución con la GPU disponible en todos los nodos comienza a mostrar mejoras significativas, especialmente al incrementar el número de hebras. Por ejemplo, con 8 nodos y 16 hebras se observa una reducción del tiempo de ejecución del 37.26\%, mientras que con 16 nodos y 16 hebras la mejora alcanza el 33.96\%. Estas diferencias son consistentes y tienden a aumentar conforme crece el número de nodos y hebras.

En configuraciones con un número elevado de nodos y hebras, la opción de permitir el acceso a la GPU en todos los nodos se presenta como claramente superior, ya que maximiza el aprovechamiento de la capacidad de cómputo distribuido y reduce de manera significativa los tiempos de ejecución.

En resumen, para experimentos pequeños o con pocos nodos ambas opciones resultan comparables. Sin embargo, en experimentos de mayor escala y con configuraciones que involucran un alto número de nodos y hebras, la estrategia más eficiente y recomendable es habilitar el uso de la GPU en todos los nodos.

\subsection{Análisis de los experimentos preliminares}

A partir de los resultados obtenidos, se recomienda fijar el número de subpoblaciones en 8, ya que este valor representa un equilibrio óptimo entre eficiencia, utilización de recursos y escalabilidad en el entorno analizado.

En cuanto al número de hebras, los datos muestran que imponer un límite estricto puede restringir el aprovechamiento total de los recursos, especialmente en configuraciones multinodo. Por ello, se recomienda no limitar el número de hebras, permitiendo que el sistema utilice tantas como sean necesarias para maximizar el rendimiento.

Respecto al uso de la GPU, los experimentos indican que en configuraciones pequeñas las diferencias entre habilitarla en todos los nodos o solo en uno son mínimas. Sin embargo, en escenarios de mayor escala, habilitar su uso en todos los nodos aporta mejoras sustanciales en el tiempo de ejecución y en la eficiencia global. Por tanto, se establece como criterio general que la GPU esté disponible en todos los nodos.

En conjunto, estas decisiones proporcionan una base sólida y coherente para el diseño de los experimentos posteriores, asegurando que se exploten al máximo los recursos disponibles sin comprometer la comparabilidad de los resultados.

\section{Pruebas mononodo}

\subsection{Ejecución en Ubuntu en nativo}

\subsubsection{CPU}

En la figura \ref{fig:single-node_ubuntu_cpu_native_time} se muestra el tiempo de ejecución para la configuración de CPU en un único nodo con Ubuntu nativo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_cpu_native_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en Ubuntu nativo (CPU).}
    \label{fig:single-node_ubuntu_cpu_native_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_cpu_native} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1               & 2515.21             & 0.00                           \\
        2               & 1253.18             & -50.18                         \\
        4               & 664.69              & -73.57                         \\
        8               & 390.72              & -84.47                         \\
        16              & 406.76              & -83.83                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en Ubuntu nativo (CPU).}
    \label{tab:single-node_ubuntu_cpu_native}
\end{table}

El tiempo de ejecución disminuye drásticamente al aumentar el número de hebras, especialmente en el rango de $1$ a $8$ hebras. Con $2$ hebras, el tiempo se reduce prácticamente a la mitad ($-50.18\%$), y con $4$ hebras, a casi una cuarta parte del tiempo original ($-73.57\%$). El mayor beneficio se observa al pasar de $4$ a $8$ hebras, alcanzando una reducción del $-84.47\%$ respecto a una hebra. Sin embargo, al incrementar a $16$ hebras, el tiempo de ejecución es ligeramente superior al obtenido con $8$ hebras, lo que sugiere la aparición de saturación o sobrecarga en el sistema. Por tanto, el óptimo se alcanza con $8$ hebras, coincidiendo con el número de núcleos físicos disponibles en el sistema.

En la figura \ref{fig:single-node_ubuntu_cpu_native_cpu} se muestra el porcentaje de uso de CPU en función del número de hebras.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_cpu_native_cpu.png}
    \caption{Uso de CPU en función del número de hebras en Ubuntu nativo (CPU).}
    \label{fig:single-node_ubuntu_cpu_native_cpu}
\end{figure}

La tabla \ref{tab:single-node_ubuntu_cpu_native_cpu} muestra el porcentaje de uso de CPU alcanzado para cada número de hebras, el máximo teórico posible (calculado como número de hebras por 100\%), y la eficiencia relativa obtenida.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Porcentaje de CPU (\%)} & \textbf{Max posible CPU (\%)} & \textbf{Eficiencia CPU (\%)} \\
        \hline
        1               & 99.00                           & 100.00                        & 99.00                        \\
        2               & 199.00                          & 200.00                        & 99.50                        \\
        4               & 395.00                          & 400.00                        & 98.75                        \\
        8               & 780.00                          & 800.00                        & 97.50                        \\
        16              & 1556.00                         & 1600.00                       & 97.25                        \\
        \hline
    \end{tabular}
    \caption{Porcentaje de uso de CPU y eficiencia en función del número de hebras en Ubuntu nativo (CPU).}
    \label{tab:single-node_ubuntu_cpu_native_cpu}
\end{table}

El uso de CPU aumenta de manera casi lineal conforme se incrementa el número de hebras, lo que evidencia un excelente escalado del paralelismo en la ejecución. La eficiencia de uso de la CPU se mantiene muy elevada en todos los casos, superando el $97\%$, lo que indica que prácticamente se está aprovechando todo el potencial de cómputo disponible. Aunque la eficiencia disminuye ligeramente al aumentar el número de hebras (desde un máximo del $99\%$ hasta un mínimo del $97.25\%$), esta reducción es mínima y esperable, ya que responde a la sobrecarga asociada a la gestión de un mayor número de hilos y a posibles contenciones internas. En conjunto, el sistema demuestra un escalado eficiente hasta $16$ hebras, con una pérdida de eficiencia muy reducida.

Estos resultados evidencian que el entorno mononodo nativo en Ubuntu gestiona eficazmente el paralelismo y aprovecha bien los recursos de la CPU, mostrando un buen escalado del rendimiento al aumentar el número de hebras.

\subsubsection{CPU + GPU}

En la figura \ref{fig:single-node_ubuntu__gpu_native_time} se muestra el tiempo de ejecución para la configuración de CPU + GPU en un único nodo con Ubuntu nativo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_gpu_native_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en Ubuntu nativo (CPU + GPU).}
    \label{fig:single-node_ubuntu__gpu_native_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_gpu_native} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1.00            & 311.68              & 0.00                           \\
        2.00            & 163.59              & -47.51                         \\
        4.00            & 84.49               & -72.89                         \\
        8.00            & 59.91               & -80.78                         \\
        16.00           & 56.45               & -81.89                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en Ubuntu nativo (CPU + GPU).}
    \label{tab:single-node_ubuntu_gpu_native}
\end{table}

El análisis de los resultados muestra que el tiempo de ejecución disminuye de forma significativa al incrementar el número de hebras, especialmente en el rango de $1$ a $4$ hebras, donde se alcanza una reducción del $72.89\%$. La mayor ganancia relativa se produce al pasar de $1$ a $2$ hebras ($-47.51\%$) y de $2$ a $4$ hebras ($-25.38\%$ adicional). Sin embargo, a partir de $8$ hebras, la mejora es marginal: el tiempo de ejecución solo disminuye de $59.91\,s$ a $56.45\,s$ al pasar de $8$ a $16$ hebras, lo que supone apenas un $1.11\%$ adicional. Esto indica que el sistema alcanza un punto óptimo de rendimiento con $8$ hebras, y que a partir de ese valor se observa una saturación en la eficiencia de la paralelización. En conjunto, la combinación CPU + GPU proporciona una aceleración notable, pero la eficiencia se estabiliza a partir de cierto número de hebras, reflejando el límite práctico de paralelismo efectivo en el entorno analizado.

\subsubsection{Comparativa CPU vs CPU + GPU}

En la figura \ref{fig:single-node_ubuntu_cpu_vs_gpu_native_time} se muestra una comparativa del tiempo de ejecución entre las configuraciones de CPU y CPU + GPU en un único nodo con Ubuntu nativo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_cpu_vs_gpu_native_time.png}
    \caption{Comparativa de tiempo de ejecución entre CPU y CPU + GPU en función del número de hebras en Ubuntu nativo.}
    \label{fig:single-node_ubuntu_cpu_vs_gpu_native_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_cpu_vs_gpu_native} se presentan los tiempos de ejecución para ambas configuraciones y la variación porcentual entre ellas.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo CPU (s)} & \textbf{Tiempo CPU+GPU (s)} & \textbf{Variación (\%)} \\
        \hline
        1               & 2515.21                 & 311.68                      & -87.61                  \\
        2               & 1253.18                 & 163.59                      & -86.95                  \\
        4               & 664.69                  & 84.49                       & -87.29                  \\
        8               & 390.72                  & 59.91                       & -84.67                  \\
        16              & 406.76                  & 56.45                       & -86.12                  \\
        \hline
    \end{tabular}
    \caption{Comparativa de tiempos de ejecución y variación porcentual entre CPU y CPU+GPU en Ubuntu nativo.}
    \label{tab:single-node_ubuntu_cpu_vs_gpu_native}
\end{table}

El análisis de los resultados muestra que el uso de GPU reduce el tiempo de ejecución entre un $84\%$ y un $88\%$ respecto a la ejecución únicamente en CPU, independientemente del número de hebras empleadas. La mejora relativa se mantiene estable al aumentar el paralelismo, lo que indica que la GPU aporta un beneficio constante y no dependiente del número de hilos de CPU utilizados. Además, mientras que en la configuración solo CPU el tiempo de ejecución deja de mejorar al pasar de $8$ a $16$ hebras (e incluso empeora ligeramente), en la configuración CPU+GPU la mejora es marginal pero consistente. En conjunto, la incorporación de GPU resulta altamente beneficiosa, acelerando el procesamiento en torno al $85\%$ en todos los escenarios analizados.

\subsection{Ejecución en contenedores de Ubuntu}
\subsubsection{CPU}

En la figura \ref{fig:single-node_ubuntu_docker_time} se muestra el tiempo de ejecución para la configuración de CPU en un único nodo con contenedores de Ubuntu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_docker_time.png}
    \caption{Tiempo de ejecución en un único nodo con contenedores de Ubuntu (CPU).}
    \label{fig:single-node_ubuntu_docker_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_docker} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1               & 2499.09             & 0.00                           \\
        2               & 1255.95             & -49.74                         \\
        4               & 652.33              & -73.90                         \\
        8               & 388.04              & -84.47                         \\
        16              & 404.09              & -83.83                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en contenedores de Ubuntu (CPU).}
    \label{tab:single-node_ubuntu_docker}
\end{table}

El tiempo de ejecución disminuye significativamente al aumentar el número de hebras, especialmente al pasar de 1 a 8 hebras, donde la reducción alcanza el 84.47\%.

La mayor ganancia relativa se obtiene al pasar de 1 a 2 hebras (-49.74\%) y de 2 a 4 hebras (-48.16\% adicional), mostrando una buena escalabilidad inicial.

A partir de 8 hebras, la mejora se estabiliza y el rendimiento apenas varía, e incluso con 16 hebras el tiempo de ejecución es ligeramente superior al de 8 hebras, lo que indica que se alcanza un límite de paralelización eficiente.

Estos resultados sugieren que, en este entorno, el uso de más de 8 hebras no aporta beneficios significativos y puede incluso generar sobrecarga, por lo que 8 hebras representa el punto óptimo de eficiencia para la ejecución en contenedores de Ubuntu con CPU.

En la figura \ref{fig:single-node_ubuntu_podman_time} se muestra el tiempo de ejecución para la configuración de CPU en un único nodo con contenedores de Ubuntu gestionados por Podman.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_podman_time.png}
    \caption{Tiempo de ejecución en un único nodo con contenedores de Ubuntu gestionados por Podman (CPU).}
    \label{fig:single-node_ubuntu_podman_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_podman} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1               & 2499.16             & 0.00                           \\
        2               & 1266.51             & -49.32                         \\
        4               & 665.38              & -73.38                         \\
        8               & 400.51              & -83.97                         \\
        16              & 413.53              & -83.45                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en contenedores de Ubuntu gestionados por Podman (CPU).}
    \label{tab:single-node_ubuntu_podman}
\end{table}

El tiempo de ejecución disminuye de forma considerable al aumentar el número de hebras, especialmente entre 1 y 8 hebras, donde la reducción alcanza el 83.97\%.

La mayor mejora relativa se observa al pasar de 1 a 2 hebras (-49.32\%) y de 2 a 4 hebras (-47.97\% adicional), lo que indica una buena escalabilidad inicial.

A partir de 8 hebras, la reducción en el tiempo de ejecución se estabiliza y el beneficio adicional es mínimo; con 16 hebras, el tiempo incluso aumenta ligeramente respecto a 8 hebras, lo que sugiere que se alcanza el límite de paralelización eficiente.

Estos resultados muestran que, en este entorno con Podman y CPU, el uso óptimo se encuentra en torno a 8 hebras, ya que aumentar más allá de este valor no aporta mejoras significativas y puede generar sobrecarga.

\subsubsection{CPU + GPU}

En la figura \ref{fig:single-node_ubuntu_docker_gpu_time} se muestra el tiempo de ejecución para la configuración de CPU + GPU en un único nodo con contenedores de Ubuntu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_docker_gpu_time.png}
    \caption{Tiempo de ejecución en un único nodo con contenedores de Ubuntu (CPU + GPU).}
    \label{fig:single-node_ubuntu_docker_gpu_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_docker_gpu} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1               & 312.18              & 0.00                           \\
        2               & 161.10              & -48.40                         \\
        4               & 85.78               & -72.52                         \\
        8               & 60.65               & -80.57                         \\
        16              & 57.45               & -81.60                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en contenedores de Ubuntu (CPU + GPU).}
    \label{tab:single-node_ubuntu_docker_gpu}
\end{table}

El uso combinado de CPU y GPU en contenedores de Ubuntu permite una reducción muy significativa en los tiempos de ejecución al aumentar el número de hebras, alcanzando una disminución del 81.60\% con 16 hebras respecto a la ejecución con una sola hebra.

La mayor ganancia relativa se observa al pasar de 1 a 2 hebras (-48.40\%) y de 2 a 4 hebras (-24.12\% adicional), lo que indica una buena escalabilidad inicial.

A partir de 8 hebras, la mejora se estabiliza y el beneficio adicional es marginal; el tiempo de ejecución con 16 hebras es solo ligeramente menor que con 8 hebras.

Estos resultados muestran que, en este entorno, la combinación de CPU y GPU permite aprovechar eficientemente el paralelismo hasta 8 hebras, siendo el punto óptimo de eficiencia, ya que aumentar más allá de este valor aporta mejoras muy pequeñas.

En la figura \ref{fig:single-node_ubuntu_podman_gpu_time} se muestra el tiempo de ejecución para la configuración de CPU + GPU en un único nodo con contenedores de Ubuntu gestionados por Podman.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_podman_gpu_time.png}
    \caption{Tiempo de ejecución en un único nodo con contenedores de Ubuntu gestionados por Podman (CPU + GPU).}
    \label{fig:single-node_ubuntu_podman_gpu_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_podman_gpu} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1               & 314.51              & 0.00                           \\
        2               & 158.84              & -49.50                         \\
        4               & 85.62               & -72.78                         \\
        8               & 59.47               & -81.09                         \\
        16              & 57.12               & -81.84                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en contenedores de Ubuntu gestionados por Podman (CPU + GPU).}
    \label{tab:single-node_ubuntu_podman_gpu}
\end{table}

El tiempo de ejecución disminuye drásticamente al aumentar el número de hebras, alcanzando una reducción del 81.84\% con 16 hebras respecto a la ejecución con una sola hebra.

La mayor mejora relativa se observa al pasar de 1 a 2 hebras (-49.50\%) y de 2 a 4 hebras (-23.28\% adicional), lo que indica una excelente escalabilidad inicial.

A partir de 8 hebras, la reducción en el tiempo de ejecución se estabiliza y el beneficio adicional es muy pequeño; el tiempo con 16 hebras es solo ligeramente menor que con 8 hebras.

Estos resultados muestran que, en este entorno con Podman y CPU+GPU, el uso óptimo se encuentra en torno a 8 hebras, ya que aumentar más allá de este valor no aporta mejoras significativas y puede generar sobrecarga.

\subsubsection{Comparativa contenedores vs nativo}

En la figura \ref{fig:single-node_ubuntu_container_vs_native_time} se muestra una comparativa del tiempo de ejecución entre las configuraciones nativas y en contenedores de Ubuntu para CPU.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_ubuntu_container_vs_native_time.png}
    \caption{Comparativa de tiempo de ejecución entre nativo y contenedores de Ubuntu en función del número de hebras, para CPU.}
    \label{fig:single-node_ubuntu_container_vs_native_time}
\end{figure}

En la tabla \ref{tab:single-node_ubuntu_container_vs_native} se presentan los tiempos de ejecución para ambas configuraciones y la variación porcentual entre ellas.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Nativo (s)} & \textbf{Docker (s)} & \textbf{Docker $\Delta$\%} & \textbf{Podman (s)} & \textbf{Podman $\Delta$\%} \\
        \hline
        1               & 311.68              & 312.18              & 0.16                       & 314.51              & 0.91                       \\
        2               & 163.59              & 161.10              & -1.52                      & 158.84              & -2.90                      \\
        4               & 84.49               & 85.78               & 1.53                       & 85.62               & 1.34                       \\
        8               & 59.91               & 60.65               & 1.24                       & 59.47               & -0.73                      \\
        16              & 56.45               & 57.45               & 1.77                       & 57.12               & 1.19                       \\
        \hline
    \end{tabular}
    \caption{Comparativa de tiempos de ejecución entre nativo, Docker y Podman (CPU+GPU) y variación porcentual respecto a nativo.}
    \label{tab:single-node_ubuntu_container_vs_native}
\end{table}

\subsection{Contenedores en contenedores de Windows}
\subsubsection{CPU}

En la figura \ref{fig:single-node_windows_docker_time} se muestra el tiempo de ejecución para la configuración de CPU en un único nodo con Docker en Windows.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_windows_docker_time.png}
    \caption{Tiempo de ejecución en un único nodo con Docker en Windows (CPU).}
    \label{fig:single-node_windows_docker_time}
\end{figure}

En la tabla \ref{tab:single-node_windows_docker_time} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1.00            & 3308.08             & 0.00                           \\
        2.00            & 1341.41             & -59.45                         \\
        4.00            & 816.06              & -75.33                         \\
        8.00            & 602.60              & -81.78                         \\
        16.00           & 453.44              & -86.29                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en Docker sobre Windows (CPU).}
    \label{tab:single-node_windows_docker_time}
\end{table}

El tiempo de ejecución disminuye de forma significativa al aumentar el número de hebras, alcanzando una reducción del 86.29\% con 16 hebras respecto a la ejecución con una sola hebra.

La mayor mejora relativa se observa al pasar de 1 a 2 hebras (-59.45\%) y de 2 a 4 hebras (-39.46\% adicional), lo que indica una excelente escalabilidad inicial.

A medida que se incrementa el número de hebras, la reducción en el tiempo de ejecución se mantiene, aunque con beneficios marginales decrecientes a partir de 8 hebras.

Estos resultados muestran que, en Docker sobre Windows (CPU), el uso de múltiples hebras es muy eficiente y permite aprovechar el paralelismo, siendo recomendable utilizar el mayor número de hebras posible para minimizar el tiempo de ejecución.

En la figura \ref{fig:single-node_windows_podman_time} se muestra el tiempo de ejecución para la configuración de CPU en un único nodo con Podman en Windows.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_windows_podman_time.png}
    \caption{Tiempo de ejecución en un único nodo con Podman en Windows (CPU).}
    \label{fig:single-node_windows_podman_time}
\end{figure}

En la tabla \ref{tab:single-node_windows_podman_time} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1.00            & 2520.21             & 0.00                           \\
        2.00            & 1359.69             & -46.05                         \\
        4.00            & 815.04              & -67.66                         \\
        8.00            & 595.54              & -76.37                         \\
        16.00           & 448.43              & -82.21                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en Podman sobre Windows (CPU).}
    \label{tab:single-node_windows_podman_time}
\end{table}

El tiempo de ejecución disminuye notablemente al aumentar el número de hebras, alcanzando una reducción del 82.21\% con 16 hebras respecto a una sola hebra.

La mayor mejora relativa se observa al pasar de 1 a 2 hebras (-46.05\%) y de 2 a 4 hebras (-21.61\% adicional), lo que indica una buena escalabilidad inicial.

A partir de 8 hebras, la reducción en el tiempo de ejecución continúa, aunque los beneficios adicionales son menores, mostrando una tendencia a estabilizarse.

Estos resultados indican que, en Podman sobre Windows (CPU), el uso de múltiples hebras es eficiente y permite aprovechar el paralelismo, siendo recomendable utilizar el mayor número de hebras posible para reducir el tiempo de ejecución, aunque las ganancias adicionales disminuyen a partir de 8 hebras.

\subsubsection{CPU + GPU}

% En la figura \ref{fig:single-node_windows_docker_gpu_time} se muestra el tiempo de ejecución para la configuración de CPU + GPU en un único nodo con Docker en Windows.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_windows_docker_gpu_time.png}
%     \caption{Tiempo de ejecución en un único nodo con Docker en Windows (CPU + GPU).}
%     \label{fig:single-node_windows_docker_gpu_time}
% \end{figure}

% En la tabla \ref{tab:single-node_windows_docker_gpu_time} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\subsection{Contenedores en contenedores de Mac}
\subsubsection{CPU}

En la figura \ref{fig:single-node_mac_docker_time} se muestra el tiempo de ejecución para la configuración de CPU en un único nodo con Docker en Mac.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_mac_docker_time.png}
    \caption{Tiempo de ejecución en un único nodo con Docker en Mac (CPU).}
    \label{fig:single-node_mac_docker_time}
\end{figure}

En la tabla \ref{tab:single-node_mac_docker_time} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1.00            & 1794.98             & 0.00                           \\
        2.00            & 951.56              & -46.99                         \\
        4.00            & 551.41              & -69.28                         \\
        8.00            & 304.35              & -83.04                         \\
        16.00           & 270.25              & -84.94                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en Docker sobre Mac (CPU).}
    \label{tab:single-node_mac_docker_time}
\end{table}

El tiempo de ejecución disminuye considerablemente al aumentar el número de hebras, alcanzando una reducción del 84.94\% con 16 hebras respecto a una sola hebra.

La mayor mejora relativa se observa al pasar de 1 a 2 hebras (-46.99\%) y de 2 a 4 hebras (-22.29\% adicional), lo que indica una buena escalabilidad inicial.

A partir de 8 hebras, la reducción en el tiempo de ejecución se estabiliza, con beneficios adicionales menores al incrementar a 16 hebras.

Estos resultados muestran que, en Docker sobre Mac (CPU), el uso de múltiples hebras es eficiente y permite aprovechar el paralelismo, siendo recomendable utilizar el mayor número de hebras posible para minimizar el tiempo de ejecución, aunque las ganancias adicionales disminuyen a partir de 8 hebras.

En la figura \ref{fig:single-node_mac_podman_time} se muestra el tiempo de ejecución para la configuración de CPU en un único nodo con Podman en Mac.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/single-node_mac_podman_time.png}
    \caption{Tiempo de ejecución en un único nodo con Podman en Mac (CPU).}
    \label{fig:single-node_mac_podman_time}
\end{figure}

En la tabla \ref{tab:single-node_mac_podman_time} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Hebras} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 hebra} \\
        \hline
        1.00            & 8247.00             & 0.00                           \\
        2.00            & 4328.00             & -47.52                         \\
        4.00            & 2250.20             & -72.71                         \\
        8.00            & 1638.79             & -80.13                         \\
        16.00           & 1625.19             & -80.29                         \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a una hebra en Podman sobre Mac (CPU).}
    \label{tab:single-node_mac_podman_time}
\end{table}

El tiempo de ejecución disminuye de forma muy significativa al aumentar el número de hebras, alcanzando una reducción del 80.29\% con 16 hebras respecto a una sola hebra.

La mayor mejora relativa se observa al pasar de 1 a 2 hebras (-47.52\%) y de 2 a 4 hebras (-25.19\% adicional), lo que indica una buena escalabilidad inicial.

A partir de 8 hebras, la reducción en el tiempo de ejecución se estabiliza, con beneficios adicionales mínimos al incrementar a 16 hebras (solo -0.16\% respecto a 8 hebras).

Estos resultados muestran que, en Podman sobre Mac (CPU), el uso de múltiples hebras es eficiente hasta cierto punto, pero las ganancias adicionales más allá de 8 hebras son muy limitadas, sugiriendo que el paralelismo óptimo se alcanza alrededor de ese valor.

\section{Pruebas multinodo}
\subsection{Ejecución en Ubuntu en nativo}
\subsubsection{CPU}

La figura \ref{fig:multi-node_ubuntu_cpu_native_time} muestra el tiempo de ejecución para la configuración de CPU en un entorno multinodo con Ubuntu nativo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_cpu_native_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en Ubuntu nativo (CPU) en entorno multinodo.}
    \label{fig:multi-node_ubuntu_cpu_native_time}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_cpu_native} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Tiempo de ejecución (s)} & \textbf{$\Delta$\% vs 1 nodo} \\
        \hline
        1              & 2515.21                          & 0.00                          \\
        2              & 2489.49                          & -1.02                         \\
        4              & 952.85                           & -62.12                        \\
        8              & 767.87                           & -69.47                        \\
        16             & 841.96                           & -66.53                        \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a un nodo en entorno multinodo Ubuntu nativo (CPU).}
    \label{tab:multi-node_ubuntu_cpu_native}
\end{table}

La tabla muestra cómo varía el tiempo de ejecución al aumentar el número de nodos en un entorno multinodo Ubuntu nativo usando CPU. Se observa que:

Al pasar de 1 a 2 nodos, la reducción del tiempo es mínima (-1.02\%), lo que indica poca mejora.
Con 4 nodos, el tiempo baja significativamente (-62.12\%), mostrando una mejora notable en la paralelización.
Con 8 nodos, la reducción es aún mayor (-69.47\%), aunque el beneficio adicional respecto a 4 nodos es menor.
Al aumentar a 16 nodos, el tiempo de ejecución aumenta ligeramente respecto a 8 nodos y la reducción porcentual disminuye (-66.53\%), lo que sugiere que a partir de cierto punto añadir más nodos no mejora el rendimiento e incluso puede empeorarlo, posiblemente por sobrecarga de comunicación o gestión.

En resumen, la eficiencia de la paralelización mejora hasta cierto número de nodos, pero después se observa un efecto de saturación o incluso degradación del rendimiento.

En la figura \ref{fig:multi-node_ubuntu_cpu_native_cpu} se muestra el porcentaje de uso de CPU y la eficiencia en función del número de nodos en un entorno multinodo con Ubuntu nativo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_cpu_native_cpu_time.png}
    \caption{Porcentaje de uso de CPU y eficiencia en función del número de nodos en Ubuntu nativo (CPU) en entorno multinodo.}
    \label{fig:multi-node_ubuntu_cpu_native_cpu}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_cpu_native_cpu} se presentan los porcentajes de uso de CPU y la eficiencia correspondiente.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Porcentaje de CPU (\%)} & \textbf{Max uso CPU (\%)} & \textbf{Eficiencia CPU (\%)} \\
        \hline
        1              & 99.00                           & 100.00                    & 99.00                        \\
        2              & 199.00                          & 200.00                    & 99.50                        \\
        4              & 399.00                          & 400.00                    & 99.75                        \\
        8              & 799.00                          & 800.00                    & 99.88                        \\
        16             & 1592.00                         & 1600.00                   & 99.50                        \\
        \hline
    \end{tabular}
    \caption{Porcentaje de uso de CPU y eficiencia en función del número de nodos en Ubuntu nativo (CPU) en entorno multinodo.}
    \label{tab:multi-node_ubuntu_cpu_native_cpu}
\end{table}

La tabla muestra el porcentaje de uso de CPU y la eficiencia al aumentar el número de nodos en un entorno multinodo Ubuntu nativo (CPU):

El uso de CPU escala casi linealmente con el número de nodos, lo que indica que los recursos se están utilizando de manera efectiva. El valor "Max uso CPU" representa el uso máximo teórico (número de nodos $\times$ 100\%), y el uso real está muy cerca de ese máximo en todos los casos. La eficiencia de CPU se mantiene muy alta (entre 99.00\% y 99.88\%) para todos los escenarios, lo que sugiere que la sobrecarga de paralelización es mínima y que el sistema aprovecha casi todo el potencial de los recursos disponibles. Solo con 16 nodos se observa una ligera caída en la eficiencia (99.50\%), pero sigue siendo excelente.

En resumen, el sistema mantiene una eficiencia de CPU muy alta al escalar el número de nodos, lo que indica una buena gestión de los recursos y una paralelización efectiva en este entorno.

\subsubsection{CPU + GPU}

La figura \ref{fig:multi-node_ubuntu_gpu_native_time} muestra el tiempo de ejecución para la configuración de CPU + GPU en un entorno multinodo con Ubuntu nativo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_gpu_native_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en Ubuntu nativo (CPU + GPU) en entorno multinodo.}
    \label{fig:multi-node_ubuntu_gpu_native_time}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_gpu_native} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 nodo} \\
        \hline
        1              & 311.68              & 0.00                          \\
        2              & 316.92              & 1.68                          \\
        4              & 322.77              & 3.56                          \\
        8              & 401.33              & 28.76                         \\
        16             & 869.40              & 178.94                        \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y variación porcentual respecto a un nodo en entorno multinodo Ubuntu nativo (CPU + GPU).}
    \label{tab:multi-node_ubuntu_gpu_native}
\end{table}

La tabla muestra los tiempos de ejecución y la variación porcentual al aumentar el número de nodos en un entorno multinodo Ubuntu nativo utilizando CPU y GPU:

Con un solo nodo, el tiempo base es de 311.68\,s. Al incrementar a 2 y 4 nodos, el tiempo de ejecución aumenta ligeramente (316.92\,s y 322.77\,s), lo que implica una pequeña penalización en lugar de una mejora (variaciones positivas del 1.68\% y 3.56\%). Con 8 nodos, el tiempo sube notablemente hasta 401.33\,s (+28.76\%), y con 16 nodos el incremento es aún mayor (869.40\,s, +178.94\%).

Estos resultados indican que, a diferencia del caso solo CPU, al añadir más nodos con GPU el rendimiento empeora progresivamente. Es probable que la sobrecarga de comunicación, la gestión de recursos compartidos o la falta de escalabilidad de la aplicación para GPU estén afectando negativamente. En este entorno, la paralelización no solo no aporta beneficios, sino que resulta contraproducente a partir de más de un nodo.

\subsubsection{Comparativa CPU vs CPU + GPU}

En la figura \ref{fig:multi-node_ubuntu_cpu_vs_gpu_native_time} se muestra una comparativa del tiempo de ejecución entre las configuraciones CPU y CPU + GPU en un entorno multinodo con Ubuntu nativo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_cpu_vs_gpu_native_time.png}
    \caption{Comparativa de tiempo de ejecución entre CPU y CPU + GPU en función del número de nodos en Ubuntu nativo en entorno multinodo.}
    \label{fig:multi-node_ubuntu_cpu_vs_gpu_native_time}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_cpu_vs_gpu_native} se presentan los tiempos de ejecución para ambas configuraciones y la variación porcentual entre ellas.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Tiempo CPU (s)} & \textbf{Tiempo CPU+GPU (s)} & \textbf{Variación (\%)} \\
        \hline
        1              & 2515.21                 & 311.68                      & -87.61                  \\
        2              & 2489.49                 & 316.92                      & -87.27                  \\
        4              & 952.85                  & 322.77                      & -66.13                  \\
        8              & 767.87                  & 401.33                      & -47.73                  \\
        16             & 841.96                  & 869.40                      & 3.26                    \\
        \hline
    \end{tabular}
    \caption{Comparativa de tiempos de ejecución entre CPU y CPU+GPU en función del número de nodos en Ubuntu nativo (multinodo) y variación porcentual.}
    \label{tab:multi-node_ubuntu_cpu_vs_gpu_native}
\end{table}

La tabla muestra que, con uno y dos nodos, la configuración CPU+GPU es considerablemente más rápida que la opción solo CPU, con reducciones de tiempo superiores al 87\%. Esto evidencia un beneficio claro del uso de GPU en escenarios con pocos nodos. Al aumentar a cuatro y ocho nodos, la ventaja de la GPU disminuye: la reducción es del 66.13\% con cuatro nodos y del 47.73\% con ocho nodos. Aunque la GPU sigue proporcionando mejores tiempos, la diferencia se reduce conforme se incrementa el número de nodos. Con dieciséis nodos, la situación se invierte y el tiempo de CPU+GPU resulta ligeramente superior al de solo CPU, con una variación positiva del 3.26\%. Esto sugiere que, a partir de cierto punto, la sobrecarga asociada al uso de GPU y la comunicación entre nodos supera los beneficios de la aceleración.

En resumen, el uso de GPU aporta grandes mejoras en los tiempos de ejecución para un bajo número de nodos, pero su escalabilidad es limitada. A medida que se incrementa el número de nodos, la eficiencia de la GPU disminuye y puede llegar a ser contraproducente, probablemente debido a la sobrecarga de comunicación y la gestión de recursos en entornos multinodo.

\subsection{Ejecución en contenedores de Ubuntu}
\subsubsection{CPU}

En la figura \ref{fig:multi-node_ubuntu_docker_time} se muestra el tiempo de ejecución para la configuración de CPU en un entorno multinodo con contenedores de Ubuntu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_docker_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en contenedores de Ubuntu (CPU) en entorno multinodo.}
    \label{fig:multi-node_ubuntu_docker_time}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_docker} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 nodo} \\
        \hline
        1              & 2499.09             & 0.00                          \\
        2              & 2484.80             & -0.57                         \\
        4              & 1054.20             & -57.82                        \\
        8              & 782.42              & -68.69                        \\
        16             & 879.20              & -64.82                        \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a un nodo en entorno multinodo con contenedores de Ubuntu (CPU).}
    \label{tab:multi-node_ubuntu_docker}
\end{table}

La tabla muestra los tiempos de ejecución y la reducción porcentual al aumentar el número de nodos en un entorno multinodo basado en contenedores de Ubuntu (CPU):

De 1 a 2 nodos, la mejora es mínima (-0.57\%), lo que indica que la paralelización apenas aporta beneficio en este rango. Con 4 nodos, la reducción es significativa (-57.82\%), mostrando una mejora clara en el rendimiento gracias a la paralelización. Con 8 nodos, la reducción alcanza el -68.69\%, lo que indica un buen aprovechamiento de los recursos al escalar. Con 16 nodos, el tiempo de ejecución aumenta ligeramente respecto a 8 nodos y la reducción porcentual disminuye a -64.82\%, lo que sugiere que a partir de cierto punto la eficiencia se ve limitada, posiblemente por la sobrecarga de coordinación y comunicación entre contenedores.

En resumen, el entorno con contenedores de Ubuntu permite una buena escalabilidad hasta 8 nodos, con mejoras notables en el tiempo de ejecución. Sin embargo, al aumentar a 16 nodos, la eficiencia disminuye, mostrando un comportamiento similar al entorno nativo: la paralelización es efectiva hasta cierto límite, tras el cual la sobrecarga afecta negativamente al rendimiento.

En la figura \ref{fig:multi-node_ubuntu_podman} se muestra el tiempo de ejecución para la configuración de CPU en un entorno multinodo con contenedores de Podman.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_podman_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en contenedores de Podman (CPU) en entorno multinodo.}
    \label{fig:multi-node_ubuntu_podman}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_podman} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 nodo} \\
        \hline
        1              & 2499.16             & 0.00                          \\
        2              & 2574.04             & 3.00                          \\
        4              & 973.92              & -61.03                        \\
        8              & 796.84              & -68.12                        \\
        16             & 870.28              & -65.18                        \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y reducción porcentual respecto a un nodo en entorno multinodo con contenedores de Podman (CPU).}
    \label{tab:multi-node_ubuntu_podman}
\end{table}

La tabla muestra cómo varía el tiempo de ejecución al aumentar el número de nodos en un entorno multinodo utilizando contenedores de Podman sobre CPU. Con un solo nodo se establece el tiempo base, mientras que al duplicar el número de nodos a dos, se observa un ligero aumento en el tiempo de ejecución, lo que indica que la paralelización no solo no aporta mejoras en este caso, sino que introduce una pequeña penalización, probablemente debida a la sobrecarga de gestión de los contenedores. Sin embargo, al incrementar a cuatro nodos, el tiempo de ejecución disminuye considerablemente, reflejando una reducción del 61.03\% respecto al caso de un nodo, lo que evidencia una mejora significativa en el rendimiento gracias a la paralelización. Esta tendencia positiva se mantiene con ocho nodos, donde la reducción alcanza el 68.12\%, mostrando un buen aprovechamiento de los recursos disponibles. No obstante, al aumentar a dieciséis nodos, el tiempo de ejecución vuelve a incrementarse ligeramente y la reducción porcentual disminuye a 65.18\%, lo que sugiere que, a partir de cierto punto, la eficiencia de la paralelización se ve limitada, posiblemente por la sobrecarga de coordinación y comunicación entre los contenedores. En conjunto, el entorno con Podman permite una escalabilidad efectiva hasta un número intermedio de nodos, pero muestra limitaciones cuando se incrementa excesivamente el grado de paralelismo.

\subsubsection{CPU + GPU}

En la figura \ref{fig:multi-node_ubuntu_docker_gpu_time} se muestra el tiempo de ejecución para la configuración de CPU + GPU en un entorno multinodo con contenedores de Ubuntu.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_docker_gpu_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en contenedores de Ubuntu (CPU + GPU) en entorno multinodo.}
    \label{fig:multi-node_ubuntu_docker_gpu_time}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_docker_gpu} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 nodo} \\
        \hline
        1              & 312.18              & 0.00                          \\
        2              & 322.77              & 3.39                          \\
        4              & 329.54              & 5.56                          \\
        8              & 388.78              & 24.54                         \\
        16             & 847.37              & 171.44                        \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y variación porcentual respecto a un nodo en entorno multinodo con contenedores de Ubuntu (CPU + GPU).}
    \label{tab:multi-node_ubuntu_docker_gpu}
\end{table}

La tabla refleja el comportamiento del tiempo de ejecución al aumentar el número de nodos en un entorno multinodo con contenedores de Ubuntu utilizando tanto CPU como GPU. Con un solo nodo, el tiempo de ejecución es el más bajo, sirviendo como referencia para el resto de configuraciones. Al incrementar a dos y cuatro nodos, lejos de mejorar, el tiempo de ejecución aumenta ligeramente, lo que se traduce en una variación porcentual positiva y evidencia que la paralelización en este contexto no resulta beneficiosa, probablemente debido a la sobrecarga de coordinación y a la gestión de recursos entre los contenedores y las GPUs. Esta tendencia se acentúa al llegar a ocho nodos, donde el tiempo de ejecución sigue incrementándose y la penalización alcanza el 24.54\%. Finalmente, con dieciséis nodos, el tiempo de ejecución se eleva de forma considerable, con una variación porcentual del 171.44\% respecto al caso de un solo nodo. Estos resultados indican que, en este entorno, la escalabilidad es muy limitada y que el uso combinado de contenedores y GPU no solo no aporta mejoras al aumentar el número de nodos, sino que puede llegar a ser claramente contraproducente, probablemente por la complejidad añadida en la gestión de los recursos y la comunicación entre nodos.

En la figura \ref{fig:multi-node_ubuntu_podman_gpu_time} se muestra el tiempo de ejecución para la configuración de CPU + GPU en un entorno multinodo con contenedores de Podman.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/cap5/multi-node_ubuntu_podman_gpu_time.png}
    \caption{Tiempo de ejecución en función del número de hebras en contenedores de Podman (CPU + GPU) en entorno multinodo.}
    \label{fig:multi-node_ubuntu_podman_gpu_time}
\end{figure}

En la tabla \ref{tab:multi-node_ubuntu_podman_gpu} se presentan los tiempos de ejecución y la reducción porcentual respecto a una hebra.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Nodos} & \textbf{Tiempo (s)} & \textbf{$\Delta$\% vs 1 nodo} \\
        \hline
        1              & 314.51              & 0.00                          \\
        2              & 316.80              & 0.73                          \\
        4              & 326.20              & 3.72                          \\
        8              & 391.86              & 24.59                         \\
        16             & 850.18              & 170.32                        \\
        \hline
    \end{tabular}
    \caption{Tiempos de ejecución y variación porcentual respecto a un nodo en entorno multinodo con contenedores de Podman (CPU + GPU).}
    \label{tab:multi-node_ubuntu_podman_gpu}
\end{table}

La tabla muestra la evolución del tiempo de ejecución al aumentar el número de nodos en un entorno multinodo con contenedores de Podman utilizando tanto CPU como GPU. Con un solo nodo, el tiempo de ejecución es el más bajo y sirve como referencia. Al pasar a dos y cuatro nodos, se observa un ligero incremento en el tiempo de ejecución, con variaciones porcentuales positivas que indican que la paralelización no aporta mejoras y, de hecho, introduce una pequeña penalización, probablemente debida a la sobrecarga de coordinación y gestión de recursos entre los contenedores y las GPUs. Esta tendencia se intensifica al aumentar a ocho nodos, donde el tiempo de ejecución crece de manera más notable y la penalización alcanza el 24.59\%. Finalmente, con dieciséis nodos, el tiempo de ejecución se incrementa considerablemente, con una variación porcentual del 170.32\% respecto al caso de un solo nodo. Estos resultados evidencian que, en este entorno, la escalabilidad es muy limitada y que el uso combinado de contenedores Podman y GPU no solo no mejora el rendimiento al aumentar el número de nodos, sino que puede resultar claramente perjudicial, probablemente debido a la complejidad añadida en la gestión de recursos y la comunicación entre nodos.

% \subsubsection{Comparativa

\subsection{Contenedores en contenedores de Windows}
\subsubsection{CPU}

\subsubsection{CPU + GPU}

\subsection{Contenedores en contenedores de Mac}
\subsubsection{CPU}

\section{Pruebas de barrido de hebras}
\subsection{Ejecución en Ubuntu en nativo}
\subsubsection{CPU}

\subsubsection{CPU + GPU}

\subsection{Ejecución en contenedores de Ubuntu}
\subsubsection{CPU}

\subsubsection{CPU + GPU}

\subsection{Contenedores en contenedores de Windows}
\subsubsection{CPU}

\subsubsection{CPU + GPU}

\subsection{Contenedores en contenedores de Mac}
\subsubsection{CPU}

